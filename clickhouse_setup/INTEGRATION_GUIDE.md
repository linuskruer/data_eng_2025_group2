# Integration Guide: Airflow + ClickHouse + dbt

## ğŸ”— How This Connects to Your Existing Code

This ClickHouse implementation seamlessly integrates with your existing Airflow pipeline to create a complete data engineering solution.

## ğŸ“Š Data Flow Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXISTING AIRFLOW PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ebay_api_scripts/dags/                                        â”‚
â”‚  â”œâ”€â”€ ebay_daily_collection.py     â”€â”€â”                          â”‚
â”‚  â”œâ”€â”€ ebay_hourly_collection.py    â”€â”€â”¤                          â”‚
â”‚  â””â”€â”€ data_manager.py              â”€â”€â”˜                          â”‚
â”‚                                                                 â”‚
â”‚  ebay_api_scripts/                                              â”‚
â”‚  â”œâ”€â”€ ebay_api_client.py          â”€â”€â”                          â”‚
â”‚  â”œâ”€â”€ weather_correlation_analyzer.py â”€â”€â”¤                        â”‚
â”‚  â””â”€â”€ weather.py                  â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEW CLICKHOUSE LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  clickhouse_setup/                                             â”‚
â”‚  â”œâ”€â”€ airflow_dags/clickhouse_data_loader.py  â† NEW DAG         â”‚
â”‚  â”œâ”€â”€ sql/bronze_tables_updated.sql          â† NEW TABLES       â”‚
â”‚  â”œâ”€â”€ dbt/models/                             â† NEW TRANSFORMS  â”‚
â”‚  â””â”€â”€ load_data_final.py                     â† DATA LOADER     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Integration Points

### **1. Airflow DAG Integration**

#### **Existing DAGs** (in `ebay_api_scripts/dags/`)
```python
# ebay_daily_collection.py - Collects eBay data daily
# ebay_hourly_collection.py - Collects eBay data hourly
# Both save data to CSV files via data_manager.py
```

#### **New DAG** (in `clickhouse_setup/airflow_dags/`)
```python
# clickhouse_data_loader.py - Loads CSV data into ClickHouse
# Runs after daily collection (schedule: '0 7 * * *')
# Depends on ebay_daily_collection DAG completion
```

### **2. Data Storage Integration**

#### **Before (CSV Only)**
```python
# data_manager.py saves to CSV
csv_file = data_manager.save_data(new_data, append=True)
# File: ../ebay_data/east_coast_weather_data.csv
```

#### **After (CSV + ClickHouse)**
```python
# CSV still created (for backup)
csv_file = data_manager.save_data(new_data, append=True)

# NEW: ClickHouse loading
client.insert_df('bronze.ebay_raw_data', df)
# Table: bronze.ebay_raw_data (partitioned, indexed)
```

### **3. Data Transformation Integration**

#### **Before (Python Only)**
```python
# weather_correlation_analyzer.py
# Basic data processing in Python
# Limited analytical capabilities
```

#### **After (Python + dbt)**
```sql
-- dbt models for advanced transformations
-- Silver layer: Clean and standardize
-- Gold layer: Business analytics
-- Automated testing and documentation
```

## ğŸš€ Step-by-Step Integration

### **Step 1: Keep Existing Airflow Running**
```bash
# Your existing setup (already running)
cd ebay_api_scripts
docker-compose up -d
```

### **Step 2: Add ClickHouse Layer**
```bash
# New ClickHouse setup
cd clickhouse_setup
docker-compose -f docker-compose-clickhouse-simple.yaml up -d
```

### **Step 3: Load Existing Data**
```bash
# Load your existing CSV data into ClickHouse
python load_data_final.py
```

### **Step 4: Set Up Automated Loading**
```bash
# Copy the new DAG to your Airflow dags folder
cp airflow_dags/clickhouse_data_loader.py ../ebay_api_scripts/dags/
```

### **Step 5: Run dbt Transformations**
```bash
# Transform Bronze â†’ Silver â†’ Gold
cd dbt
dbt run
```

## ğŸ“‹ Modified Files and New Files

### **Files You Already Have** (No Changes Needed)
```
ebay_api_scripts/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ ebay_daily_collection.py     âœ… Keep as-is
â”‚   â””â”€â”€ ebay_hourly_collection.py    âœ… Keep as-is
â”œâ”€â”€ ebay_api_client.py               âœ… Keep as-is
â”œâ”€â”€ data_manager.py                  âœ… Keep as-is
â”œâ”€â”€ weather_correlation_analyzer.py  âœ… Keep as-is
â””â”€â”€ docker-compose.yaml              âœ… Keep as-is
```

### **New Files Added** (ClickHouse Integration)
```
clickhouse_setup/
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ clickhouse_data_loader.py    ğŸ†• NEW: Loads data to ClickHouse
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ bronze_tables_updated.sql    ğŸ†• NEW: ClickHouse schema
â”‚   â””â”€â”€ analytical_queries.sql       ğŸ†• NEW: Sample analytics
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/silver/               ğŸ†• NEW: Data cleaning
â”‚   â”œâ”€â”€ models/gold/                 ğŸ†• NEW: Business analytics
â”‚   â””â”€â”€ dbt_project.yml              ğŸ†• NEW: dbt configuration
â”œâ”€â”€ docker-compose-clickhouse-simple.yaml ğŸ†• NEW: ClickHouse setup
â””â”€â”€ load_data_final.py               ğŸ†• NEW: Data loader
```

## ğŸ”§ Configuration Changes

### **Airflow Configuration** (Minimal Changes)
```yaml
# Add to your existing docker-compose.yaml
volumes:
  - ./clickhouse_setup/airflow_dags:/opt/airflow/dags/clickhouse
  - ./clickhouse_setup:/opt/airflow/clickhouse_setup
```

### **ClickHouse Configuration** (New)
```yaml
# clickhouse_setup/docker-compose-clickhouse-simple.yaml
services:
  clickhouse-server:
    image: clickhouse/clickhouse-server:latest
    ports:
      - "8123:8123"
      - "9000:9000"
```

## ğŸ“Š Data Comparison

### **Before Integration**
```
Data Sources â†’ Airflow â†’ CSV Files
â”œâ”€â”€ eBay API data: 113 records
â”œâ”€â”€ Weather data: 672 records
â””â”€â”€ Storage: CSV files only
```

### **After Integration**
```
Data Sources â†’ Airflow â†’ CSV Files + ClickHouse
â”œâ”€â”€ eBay API data: 113 records â†’ 226 ClickHouse records
â”œâ”€â”€ Weather data: 672 records â†’ 672 ClickHouse records
â”œâ”€â”€ Storage: CSV files (backup) + ClickHouse (analytics)
â””â”€â”€ Transformations: Bronze â†’ Silver â†’ Gold layers
```

## ğŸ”„ Automated Workflow

### **Daily Schedule**
```
06:00 AM - ebay_daily_collection.py runs
07:00 AM - clickhouse_data_loader.py runs (loads new data)
08:00 AM - dbt models run (transforms Bronze â†’ Silver â†’ Gold)
```

### **Hourly Schedule**
```
Every hour - ebay_hourly_collection.py runs
Every hour - Incremental ClickHouse updates
```

## ğŸ§ª Testing the Integration

### **1. Test Data Flow**
```bash
# Check Airflow is collecting data
docker exec ebay_api_scripts-airflow-scheduler-1 airflow dags list

# Check ClickHouse has data
docker exec clickhouse-server clickhouse-client --query "SELECT COUNT(*) FROM bronze.ebay_raw_data"
```

### **2. Test Analytics**
```bash
# Run sample analytical queries
Get-Content clickhouse_setup/sql/analytical_queries.sql | docker exec -i clickhouse-server clickhouse-client --multiquery
```

### **3. Test dbt Models**
```bash
cd clickhouse_setup/dbt
dbt run --models silver
dbt test
```

## ğŸš¨ Troubleshooting Integration

### **Common Issues**

#### **1. Airflow Can't Find ClickHouse**
```bash
# Check if ClickHouse is running
docker ps | grep clickhouse

# Check network connectivity
docker exec ebay_api_scripts-airflow-scheduler-1 ping clickhouse-server
```

#### **2. Data Not Loading**
```bash
# Check data quality logs
docker exec clickhouse-server clickhouse-client --query "SELECT * FROM bronze.data_quality_logs ORDER BY log_timestamp DESC LIMIT 5"
```

#### **3. dbt Connection Issues**
```bash
# Test dbt connection
cd clickhouse_setup/dbt
dbt debug
```

## ğŸ“ˆ Benefits of Integration

### **Before (CSV Only)**
- âŒ Limited analytical capabilities
- âŒ No data quality monitoring
- âŒ Manual data processing
- âŒ No incremental updates
- âŒ Poor query performance

### **After (CSV + ClickHouse + dbt)**
- âœ… Advanced analytical queries
- âœ… Automated data quality monitoring
- âœ… Automated transformations
- âœ… Incremental updates
- âœ… High-performance analytics
- âœ… Data lineage tracking
- âœ… Automated testing
- âœ… Documentation generation

## ğŸ¯ Next Steps

1. **Deploy Integration**: Run the setup scripts
2. **Monitor Performance**: Check data quality logs
3. **Run Analytics**: Execute analytical queries
4. **Scale Up**: Add more data sources
5. **Automate**: Set up monitoring dashboards

---

**Result**: Your existing Airflow pipeline now feeds into a powerful ClickHouse data warehouse with dbt transformations, creating a complete data engineering solution! ğŸš€
