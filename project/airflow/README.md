# ğŸŒ¤ï¸ Airflow Weather Data Ingestion Pipeline

## ğŸš€ Overview
This project sets up an **Apache Airflow pipeline** using **Docker Compose** to automatically ingest and validate weather data from multiple East Coast cities via the **Open-Meteo API**.  

Itâ€™s designed as part of a modular data platform that can later feed data into **ClickHouse** and **dbt** for analytics.

---

## ğŸ“‚ Project Structure
```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ weather_ingestion_dag.py      # Main Airflow DAG
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ weather_api.py            # Weather ingestion script
â”‚   â””â”€â”€ data/                         # CSV outputs (saved here)
â”œâ”€â”€ logs/                             # Airflow logs
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ docker-compose.yml                # Airflow setup
â””â”€â”€ README.md                         # This file
```

---

## âš™ï¸ Prerequisites
Make sure you have:
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Python 3.8+](https://www.python.org/downloads/) (optional, for local testing)

---

## ğŸ§© Setup Instructions


### 1ï¸âƒ£ Initialize Airflow
This sets up the Airflow database, installs requirements, and creates the admin user.

```bash
docker compose up airflow-init
```

Wait for the message:
```
User admin created
Initialization complete
```

### 2ï¸âƒ£ Start all Airflow services
```bash
docker compose up -d
```

This starts:
- **airflow-webserver** (UI)
- **airflow-scheduler**
- **airflow-db** (Postgres metadata DB)

Check running containers:
```bash
docker ps
```

---

## ğŸŒ Airflow Web Interface
Go to:  
ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Login:
```
Username: admin
Password: admin
```

---

## ğŸ§  Running the Pipeline

1. In the Airflow UI, find your DAG:
   ```
   weather_data_ingestion
   ```
2. Toggle it **ON** (switch turns green).  
3. Click â–¶ï¸ to **trigger** it manually or wait for its daily schedule.

---

## ğŸ“Š Output
After the DAG runs, youâ€™ll find:
```
./dags/data/east_coast_weather_all_cities.csv
```

Each record includes:
- weather_code  
- temperature  
- humidity  
- cloudcover  
- rain  
- sunshine duration  
- wind speed  
- postal code / prefix / city  

Cities included:
- New York  
- Philadelphia  
- Boston  
- Jacksonville  
- Miami  

---

## ğŸ§ª Data Quality Checks
The DAG performs automatic validation:
- Checks for **missing (null)** values  
- Detects **duplicate** rows  

If problems are found, the task fails and retries automatically.

---

## ğŸ§° Useful Commands

**Restart everything**
```bash
docker compose down && docker compose up -d
```

**Re-initialize Airflow**
```bash
docker compose up airflow-init
```

**View logs**
```bash
docker compose logs -f airflow-webserver
```

**Stop all containers**
```bash
docker compose down
```

---

## ğŸ§¼ .gitignore Example
Add this to your `.gitignore` file:
```
# Airflow & Docker files
logs/
__pycache__/
*.pyc
*.pid
*.db

# Data outputs
dags/data/*.csv

# System files
.env
.DS_Store
```

---

## ğŸ§­ Next Steps
Once ingestion is stable:
- Connect **ClickHouse** to load the data from `/dags/data/`
- Use **dbt** for transformations and analytics

---

âœ… **Youâ€™re ready!**  
Run the DAG, check your data, and enjoy an automated weather ingestion workflow built with Airflow.
